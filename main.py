import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ==========================================
# 0. è³‡æ–™æº–å‚™èˆ‡ã€Œè²¼æ¨™ç±¤ã€ (æ–°å¢éƒ¨åˆ†)
# ==========================================
# è®€å–ç‰¹å¾µæª”æ¡ˆ
df = pd.read_csv('data/features/account_features_v1.csv')
print(f"è¼‰å…¥ä¸»è³‡æ–™å®Œæˆï¼Œç¸½æ¨£æœ¬æ•¸: {len(df):,}")

# è®€å–è­¦ç¤ºèˆ‡é æ¸¬å¸³æˆ¶åå–®
try:
    df_alert = pd.read_csv('data/åˆè³½è³‡æ–™/acct_alert.csv')
    df_predict = pd.read_csv('data/åˆè³½è³‡æ–™/acct_predict.csv')
    
    # å»ºç«‹ä¸€å€‹é›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    alert_set = set(df_alert['acct'])
    predict_set = set(df_predict['acct'])
    
    # å®šç¾©è²¼æ¨™ç±¤å‡½å¼ (å„ªå…ˆç´š: Alert > Predict > Normal)
    def categorize_account(acct):
        if acct in alert_set:
            return 'Alert'
        elif acct in predict_set:
            return 'Predict'
        else:
            return 'Normal'
            
    # æ‡‰ç”¨åˆ°ä¸»è³‡æ–™æ¡† (å‡è¨­ä½ çš„ä¸»éµæ˜¯ from_acct)
    print("æ­£åœ¨æ¨™è¨˜å¸³æˆ¶é¡å‹ (Alert/Predict/Normal)...")
    df['account_type'] = df['from_acct'].apply(categorize_account)
    
    print("\nå¸³æˆ¶é¡å‹çµ±è¨ˆï¼š")
    print(df['account_type'].value_counts())
    
except Exception as e:
    print(f"âš ï¸ è®€å–è­¦ç¤º/é æ¸¬æª”å¤±æ•—æˆ–æ¬„ä½å°ä¸ä¸Šï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘ã€‚éŒ¯èª¤: {e}")
    # å¦‚æœè®€å¤±æ•—ï¼Œå°±å…¨è¨­ç‚º Normal ä»¥å…ç¨‹å¼æ›æ‰
    df['account_type'] = 'Normal'

# ==========================================
# 1. ç‰¹å¾µå·¥ç¨‹ (ç¶­æŒåŸæ¨£)
# ==========================================
features = [
    # åŸºæœ¬äº¤æ˜“è¦æ¨¡èˆ‡æ´»èºåº¦
    'txn_cnt', 'active_days', 'txn_cnt_per_day',
    # äº¤æ˜“é‡‘é¡åˆ†ä½ˆç‰¹å¾µ
    'mean_amt', 'std_amt', 'p95_amt',
    # äº¤æ˜“é¡å‹èˆ‡è¡Œç‚ºç‰¹å¾µ
    'self_txn_ratio', 'cross_bank_ratio', 'night_txn_ratio', 'foreign_currency_ratio',
    # æ™‚é–“è¡Œç‚ºç‰¹å¾µ
    'min_txn_gap', 'std_txn_gap', 'max_txn_per_day',
    # äº¤æ˜“é€šè·¯ä½¿ç”¨æ¯”ä¾‹ç‰¹å¾µ
    'atm_ratio', 'counter_ratio', 'mobile_bank_ratio', 'web_bank_ratio',
    'voice_ratio', 'eatm_ratio', 'epay_ratio', 'system_txn_ratio', 'unk_channel_ratio',
    # é€šè·¯é›†ä¸­åº¦ç‰¹å¾µ
    'channel_entropy'
]

df_model = df[features].fillna(0)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_model)

# è¨­å®š K çš„ç¯„åœï¼Œä¾‹å¦‚å¾ 2 åˆ° 15
k_range = range(2, 16)
sse = []

for k in k_range:
    # ä½¿ç”¨ MiniBatchKMeans åŠ å¿«å¤§æ•¸æ“šçš„é‹ç®—é€Ÿåº¦
    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=4096, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    sse.append(kmeans.inertia_) # Inertia å°±æ˜¯ SSE (ç¾¤å…§èª¤å·®å¹³æ–¹å’Œ)
    print(f"å·²è¨ˆç®— K={k}, SSE={kmeans.inertia_:.2f}")

# ç¹ªè£½æ‰‹è‚˜åœ–
plt.figure(figsize=(10, 6))
plt.plot(k_range, sse, marker='o', linestyle='--')
plt.title('Elbow Method Analysis')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('SSE (Inertia)')
plt.xticks(k_range)
plt.grid(True)
plt.show()

# ==========================================
# 2. åŸ·è¡Œåˆ†ç¾¤ (ä½¿ç”¨ä½ æ±ºå®šçš„ K=7)
# ==========================================
best_k = 9
print(f"\n>>> ä½¿ç”¨ K={best_k} é€²è¡Œ Mini-Batch K-Means åˆ†ç¾¤...")

final_model = MiniBatchKMeans(
    n_clusters=best_k, 
    batch_size=8192,  # åŠ å¤§æ‰¹æ¬¡ï¼Œæ¸›å°‘éœ‡ç›ª (åŸæœ¬ 4096)
    n_init=20,        # å¤šè©¦å¹¾æ¬¡èµ·é»ï¼Œç¢ºä¿æ‰¾åˆ°æœ€ä½³è§£ (åŸæœ¬ 10)
    max_no_improvement=20, # (é€²éš) å¦‚æœé€£çºŒ 20 æ¬¡æ²’è®Šå¥½å°±ææ—©åœ
    random_state=42
)
labels = final_model.fit_predict(df_scaled)
df['cluster'] = labels

# ==========================================

# 3. è¦–è¦ºåŒ–çµæœ (PCA + Heatmap)

# ==========================================



# (A) PCA æ•£ä½ˆåœ–

pca = PCA(n_components=2)
components = pca.fit_transform(df_scaled)
plt.figure(figsize=(10, 6))
scatter = plt.scatter(components[:, 0], components[:, 1], c=labels, cmap='tab10', alpha=0.5, s=10)
plt.colorbar(scatter, label='Cluster ID')
plt.title(f'Cluster Visualization (K={best_k})')
plt.show()



# (B) ç‰¹å¾µç†±åŠ›åœ–

cluster_summary = pd.DataFrame(df_scaled, columns=features)
cluster_summary['cluster'] = labels
cluster_means = cluster_summary.groupby('cluster').mean()



plt.figure(figsize=(14, 8))
sns.heatmap(cluster_means, annot=True, cmap='RdBu_r', center=0, fmt='.2f')
plt.title(f'Cluster Feature Heatmap (K={best_k})')
plt.xticks(rotation=45, ha='right')
plt.show()



# ==========================================
# 4. çµ±è¨ˆåˆ†æçµæœ (ä¾›AIåˆ†æä½¿ç”¨)
# ==========================================
print("\n" + "="*80)
print("Cluster Analysis Statistics")
print("="*80)

# å„ç¾¤å¤§å°
print("\n1. Cluster Size Distribution:")
print("-"*80)
cluster_counts = df['cluster'].value_counts().sort_index()
for cluster_id, count in cluster_counts.items():
    pct = count / len(df) * 100
    print(f"Cluster {cluster_id}: {count:,} accounts ({pct:.2f}%)")

# ä½¿ç”¨åŸå§‹æ•¸å€¼ï¼ˆæœªæ¨™æº–åŒ–ï¼‰é€²è¡Œçµ±è¨ˆ
df_original = df[features + ['cluster']].copy()

print("\n2. Cluster Feature Means (Original Scale):")
print("-"*80)
cluster_means_original = df_original.groupby('cluster')[features].mean()
print(cluster_means_original.round(4).to_string())

print("\n3. Cluster Feature Medians (Original Scale):")
print("-"*80)
cluster_medians_original = df_original.groupby('cluster')[features].median()
print(cluster_medians_original.round(4).to_string())

print("\n4. Cluster Feature Std (Original Scale):")
print("-"*80)
cluster_std_original = df_original.groupby('cluster')[features].std()
print(cluster_std_original.round(4).to_string())

# PCAè§£é‡‹è®Šç•°æ•¸
print("\n5. PCA Explained Variance:")
print("-"*80)
print(f"PC1: {pca.explained_variance_ratio_[0]:.4f} ({pca.explained_variance_ratio_[0]*100:.2f}%)")
print(f"PC2: {pca.explained_variance_ratio_[1]:.4f} ({pca.explained_variance_ratio_[1]*100:.2f}%)")
print(f"Total: {sum(pca.explained_variance_ratio_):.4f} ({sum(pca.explained_variance_ratio_)*100:.2f}%)")

# Standardizedç‰¹å¾µå‡å€¼ï¼ˆHeatmapé¡¯ç¤ºçš„æ•¸å€¼ï¼‰
print("\n6. Cluster Feature Means (Standardized - as shown in Heatmap):")
print("-"*80)
print(cluster_means.round(4).to_string())

# ==========================================
# 5. é—œéµåˆ†æï¼šè­¦ç¤ºå¸³æˆ¶éƒ½åœ¨å“ªä¸€ç¾¤ï¼Ÿ (æ–°å¢éƒ¨åˆ†)
# ==========================================
print("\n" + "="*80)
print("ğŸ”¥ğŸ”¥ğŸ”¥ è­¦ç¤ºå¸³æˆ¶è½é»åˆ†æ (Risk Analysis) ğŸ”¥ğŸ”¥ğŸ”¥")
print("="*80)

# 1. è£½ä½œäº¤å‰è¡¨ (æ¯å€‹ Cluster æœ‰å¤šå°‘ Alert, Normal, Predict)
cross_tab = pd.crosstab(df['cluster'], df['account_type'])

# 2. è¨ˆç®—ã€Œè­¦ç¤ºå¸³æˆ¶æ¿ƒåº¦ (Alert Rate)ã€
# é€™ä»£è¡¨ï¼šåœ¨è©²ç¾¤è£¡ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹æ˜¯è­¦ç¤ºå¸³æˆ¶ï¼Ÿ(æ•¸å€¼è¶Šé«˜è¶Šå±éšª)
if 'Alert' in cross_tab.columns:
    cross_tab['Total'] = cross_tab.sum(axis=1)
    cross_tab['Alert_Rate(%)'] = (cross_tab['Alert'] / cross_tab['Total'] * 100).round(2)
    
    # ä¾ç…§å±éšªç¨‹åº¦æ’åºé¡¯ç¤º
    risk_report = cross_tab.sort_values('Alert_Rate(%)', ascending=False)
    print("\nä¾ã€Œè­¦ç¤ºå¸³æˆ¶æ¿ƒåº¦ã€æ’åºçš„é¢¨éšªç¾¤èšè¡¨ï¼š")
    print(risk_report)
    
    # 3. ç•«å‡ºé¢¨éšªåœ–
    plt.figure(figsize=(12, 6))
    
    # é›™è»¸åœ–ï¼šå·¦è»¸æ˜¯äººæ•¸é•·æ¢åœ–ï¼Œå³è»¸æ˜¯é¢¨éšªç‡æŠ˜ç·šåœ–
    ax1 = plt.gca()
    ax2 = ax1.twinx()
    
    # ä¾ç…§ Cluster ID æ’åºæ–¹ä¾¿çœ‹
    chart_data = cross_tab.sort_index()
    
    # ç•«é•·æ¢åœ– (è©²ç¾¤ç¸½äººæ•¸)
    chart_data[['Normal', 'Predict', 'Alert']].plot(kind='bar', stacked=True, ax=ax1, colormap='Pastel1')
    
    # ç•«æŠ˜ç·šåœ– (è­¦ç¤ºå¸³æˆ¶æ¿ƒåº¦)
    ax2.plot(chart_data.index, chart_data['Alert_Rate(%)'], color='red', marker='o', linewidth=2, label='Alert Rate (%)')
    
    ax1.set_ylabel('Number of Accounts')
    ax2.set_ylabel('Alert Rate (%) (Red Line)', color='red')
    ax1.set_title('Cluster Composition & Risk Level Analysis')
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    
    plt.show()
    
    # 4. è‡ªå‹•åˆ¤è®€çµè«–
    riskiest_cluster = risk_report.index[0]
    highest_rate = risk_report.iloc[0]['Alert_Rate(%)']
    print(f"\n>>> çµè«–ï¼šæœ€å±éšªçš„ç¾¤èšæ˜¯ ã€Cluster {riskiest_cluster}ã€‘")
    print(f"    å®ƒçš„è­¦ç¤ºå¸³æˆ¶ä½”æ¯”é«˜é” {highest_rate}%ã€‚")
    print(f"    è«‹ç‰¹åˆ¥æª¢æŸ¥é€™ä¸€ç¾¤çš„ç‰¹å¾µ (åƒè€ƒå‰é¢çš„ Heatmap)ï¼Œé€™å°±æ˜¯è©æ¬ºçŠ¯çš„é•·ç›¸ï¼")
    
    # 5. æª¢æŸ¥ Predict å¸³æˆ¶
    if 'Predict' in cross_tab.columns:
        predict_in_risk_cluster = cross_tab.loc[riskiest_cluster, 'Predict']
        print(f"\n>>> é æ¸¬å»ºè­°ï¼š")
        print(f"    åœ¨æœ€å±éšªçš„ Cluster {riskiest_cluster} ä¸­ï¼Œç™¼ç¾äº† {predict_in_risk_cluster} å€‹ã€Œé æ¸¬å¸³æˆ¶(Predict)ã€ã€‚")
        print(f"    é€™äº›å¸³æˆ¶éå¸¸å¯èƒ½ä¹Ÿæ˜¯ç•°å¸¸å¸³æˆ¶ï¼Œå»ºè­°å„ªå…ˆé€šå ±æˆ–å¯©æŸ¥ï¼")

else:
    print("è³‡æ–™ä¸­æ²’æœ‰ç™¼ç¾ 'Alert' æ¨™ç±¤ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªç‡ã€‚")

print("="*80)